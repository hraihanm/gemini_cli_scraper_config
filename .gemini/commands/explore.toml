description = "Rapid CSV-based parser generation and testing for web scraping"
prompt = """
You are now in **Exploration Mode** - a specialized workflow for rapid CSV-based parser generation and testing.

## Mode Overview
**Purpose**: Rapid CSV-based parser generation and testing
**Best For**: Quick prototyping, CSV-driven development, rapid iteration
**Time Target**: Complete scraper generation within 30 minutes

## Workflow Protocol

### Phase 1: CSV Analysis & Site Discovery
1. **Parse CSV Specification**: Analyze the provided CSV file to understand required fields
2. **Site Structure Analysis**: Use browser tools to understand the target website architecture  
3. **Standard Pagination Detection**: Look for visible pagination buttons, page numbers, and "Next" links first
4. **Subcategory Discovery**: Ensure ALL subcategories and nested categories are discovered
5. **Field Mapping**: Map CSV fields to website elements using browser verification
6. **Page Type Identification**: Determine required page types (listings, details, categories)
7. **Network Request Analysis**: ONLY as last resort when standard pagination methods fail

### Phase 2: Rapid Parser Generation
1. **Seeder Creation**: Generate minimal seeder based on site structure
2. **Parser Development**: Create parsers for each identified page type
3. **Selector Verification**: Use browser tools to verify all selectors before implementation
4. **Immediate Testing**: Test each parser as soon as it's created

### Phase 3: Integration & Validation
1. **Data Flow Testing**: Ensure proper variable passing between parsers
2. **CSV Output Validation**: Verify extracted data matches CSV specification
3. **Edge Case Handling**: Test with missing elements and error conditions
4. **Performance Optimization**: Optimize for speed and reliability

## Required Tools & Techniques

### Browser-First Analysis (MANDATORY)
```javascript
// Always start with browser navigation and analysis
browser_navigate('{{args}}')
browser_snapshot()

// STEP 1: Look for standard pagination indicators first
browser_evaluate(() => {
    const paginationButtons = document.querySelectorAll('.pagination a, .load-more, .next-page, .show-more, .page-numbers a')
    return Array.from(paginationButtons).map(btn => ({
        text: btn.textContent.trim(),
        href: btn.href,
        classes: btn.className,
        type: 'standard_pagination'
    }))
})

// STEP 2: Check for infinite scroll indicators
browser_evaluate(() => {
    const scrollElements = document.querySelectorAll('[data-infinite-scroll], .infinite-scroll, .load-more-btn')
    return Array.from(scrollElements).map(el => ({
        text: el.textContent.trim(),
        classes: el.className,
        type: 'infinite_scroll'
    }))
})

// STEP 3: Discover all subcategories
browser_evaluate(() => {
    const subcategoryLinks = document.querySelectorAll('.subcategory a, .category-item a, .menu-item a')
    return Array.from(subcategoryLinks).map(link => ({
        text: link.textContent.trim(),
        href: link.href,
        level: link.closest('.submenu, .dropdown') ? 2 : 1
    }))
})

browser_inspect_element('Element description', 'ref')
browser_verify_selector('Element', 'selector', 'expected')

// LAST RESORT: Network request analysis ONLY if standard pagination fails
// browser_network_requests()

// LAST RESORT: Only download HTML pages if auto_download fails
// browser_download_page('page-name.html')
```

### Pagination Strategy (Progressive Approach)

**STEP 1: Standard Pagination Detection**
- Look for visible pagination buttons, page numbers, "Next" links
- Check for "Load More" buttons or similar UI elements
- Use these methods FIRST as they are most reliable

**STEP 2: Infinite Scroll Detection**
- Check for infinite scroll indicators
- Look for scroll-triggered loading mechanisms
- Test if scrolling reveals more content

**STEP 3: Network Request Analysis (LAST RESORT)**
- ONLY use `browser_network_requests()` when:
  - No visible pagination buttons found
  - Standard pagination methods don't work
  - Need to find API-driven pagination patterns
  - Infinite scroll requires API calls to load more content

**When to Use Network Analysis**:
- ✅ No visible pagination buttons or page numbers
- ✅ "Load More" buttons don't work or are missing
- ✅ Infinite scroll requires API calls to load content
- ✅ Need to find hidden pagination parameters

**When NOT to Use Network Analysis**:
- ❌ Visible pagination buttons work fine
- ❌ Standard "Next" links are available
- ❌ Page numbers are clearly visible and functional
- ❌ "Load More" buttons work as expected

### Immediate Parser Testing (MANDATORY)
```javascript
// PREFERRED: Test with auto_download (most efficient)
parser_tester({
  scraper_dir: "D:\\DataHen\\projects\\playwright-mcp-mod\\generated_scraper\\[scraper_name]",
  parser_path: "parsers/details.rb",
  auto_download: true,
  page_type: "details",
  quiet: false
})

// LAST RESORT: Only use html_file if auto_download fails
// parser_tester({
//   scraper_dir: "D:\\DataHen\\projects\\playwright-mcp-mod\\generated_scraper\\[scraper_name]",
//   parser_path: "parsers/details.rb",
//   html_file: "D:\\DataHen\\projects\\playwright-mcp-mod\\cache\\product-detail.html",
//   page_type: "details",
//   quiet: false
// })
```

## Quality Standards for Exploration Mode

### Speed Requirements
- **Site Analysis**: Complete within 5 minutes
- **Parser Generation**: Generate all parsers within 10 minutes  
- **Initial Testing**: Test all parsers within 15 minutes
- **Total Cycle**: Complete exploration within 30 minutes

### Reliability Standards
- **Selector Accuracy**: >90% match rate using browser_verify_selector
- **Data Extraction**: >95% of required CSV fields successfully extracted
- **Pagination Coverage**: ALL pages and subcategories must be accessed
- **Standard Pagination First**: Use visible pagination buttons, page numbers, and "Next" links before network analysis
- **Network Analysis Last Resort**: Only use browser_network_requests when standard pagination methods fail
- **Error Handling**: Graceful handling of missing elements
- **Variable Passing**: Proper context preservation throughout pipeline

## Working Directory
All development must happen in `./generated_scraper/[scraper_name]/`

## CSV Specification Integration
When provided with a CSV spec file, you will:
- Parse the `column_name`, `column_type`, and `dev_notes` fields
- Map `FIND` operations to CSS selector extraction logic
- Implement `PROCESS` operations with appropriate business logic
- Handle data type conversions (str, float, boolean) correctly
- Include comprehensive error handling for missing fields

## Expected Output
- **Working Scraper**: Fully functional scraper ready for deployment
- **CSV Compliance**: All extracted data matches CSV specification
- **Documentation**: Clear comments explaining selector choices and business logic
- **Test Results**: Comprehensive test results for all parsers

## Integration with System.md
Follow all operational rules from system.md:
- Use `parser_tester` MCP tool for ALL parser testing
- Follow mandatory selector verification protocol
- Implement robust variable passing and context management
- Include comprehensive error handling requirements

Now, let's begin exploration mode with the target URL: {{args}}
"""
